_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
simple_rnn_1 (SimpleRNN)     (None, 7)                 63
_________________________________________________________________
dense_1 (Dense)              (None, 4)                 32
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 5
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_1 (LSTM)                (None, 7)                 252
_________________________________________________________________
dense_1 (Dense)              (None, 4)                 32
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 5
=================================================================
Total params: 289
Trainable params: 289
Non-trainable params: 0

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
gru_1 (GRU)                  (None, 7)                 189
_________________________________________________________________
dense_1 (Dense)              (None, 4)                 32
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 5
=================================================================
Total params: 226
Trainable params: 226
Non-trainable params: 0

Simple RNN 의 경우 

gate가 없으므로 1을 곱해줘서 (input + biases +output)*1*output 해서 (1+1+7)*1*7 =63 params가 나왔고

GRU의 경우 
LSTM의 모델에서 cell-state를 제거하고 hidden state를 사용한 모형으로 LSTM모형을 약간 축소한 모형입니다.
또한 2가지 게이트가 있는데 reset gate와 update gate가 있습니다. 
reset gate의 경우 lstm의 forget, input gate 역활을 하며 update gate는 얼마나 많이 과거정보를 잊어야 하는지 결정하는데 사용합니다.
그래서 (input + biases +output)*3*output 을 해서 (1+1+7)*3*7= 189 가 나왔습니다.

LSTM의 경우 params가 252에 반해 GRU는 189입니다. 
GRU는 약간 더 적은 작동을 해서 LSTM 보다 속도가 조금 더 빠릅니다. 그렇다고 해서 LSTM 과 GRU 둘중에 뭐가 더 좋다고 확실하게 말할수 없고
경우에 따라서 무엇을 쓸지 무엇이 더 좋은지 판단을 해야하고 보통 둘다 사용한다고 합니다.